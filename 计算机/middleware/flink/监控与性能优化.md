本文介绍下flink的监控。

Flink Metrics 是 Flink 集群运行中的各项指标，包含机器系统指标，比如：CPU、内存、线程、JVM、网络、IO、GC 以及任务运行组件（JM、TM、Slot、作业、算子）等相关指标。这些指标在flink ui可以直观看到。

FlinkUI为flink系统自带（这些监控指标也可以通过restful api拉取），不在赘述。

# 自定义监控
除了自带的监控，用户也可以自定义监控指标。可以在RichFunction中调用如下代码获取MetricGroup，然后注册一个指标，最后在你需要的场景使用该指标即可。目前Flink支持Counters、Gauges、Histograms、Meters四种类型的监控指标。其中counter、guage比较常用

## Counter
```scala
class MyMapper extends RichMapFunction[Long, Long]{
    @transient private var counter: Counter = _
    override def open(parameters: Configuration): Unit = {
        counter =  getRuntimeContext().getMetricGroup().counter("your_count_name")
    }

    override def map(input: Long): Long = {
        counter.incr()
        ...do your work..
    }
}
```
## Gauges
Gauge 是最简单的 Metrics ，它反映一个指标的瞬时值。比如要看现在 TaskManager 的 JVM heap 内存用了多少，就可以每次实时的暴露一个 Gauge，Gauge 当前的值就是 heap 使用的量。

使用前首先创建一个实现 org.apache.flink.metrics.Gauge 接口的类。返回值的类型没有限制。您可以通过在 MetricGroup 上调用 gauge

```scala
class MyMapper extends RichMapFunction[Long, Long]{
    @transient private var countValue  = 0
    override def open(parameters: Configuration): Unit = {
         getRuntimeContext()
            .getMetricGroup()
            .gauge[Int, ScalaGauge[Int]]("your_gauges_name", ScalaGauge[Int](
                () =>countValue)
            )
    }

    override def map(input: Long): Long = {
        countValue+=1
        ...do your work..
    }
}
```

## Hstograms
Histogram 用于统计一些数据的分布，比如说 Quantile、Mean、StdDev、Max、Min 等，其中最重要一个是统计算子的延迟。此项指标会记录数据处理的延迟信息，对任务监控起到很重要的作用。

flink中没有默认的Histograms实现类，需要自行实现或者引入Codahale/DropWizard Histograms来完成

```scala
class MyMapper extends RichMapFunction[Long, Long]{
    @transient private var histogram: Histogram  = _
    override def open(parameters: Configuration): Unit = {
        val dropwizardHistogram =  new com.codahale.metrics.Histogram(new SlidingWindowReservoir(500))
        meter =  getRuntimeContext()
            .getMetricGroup()
            .histogram("your_histogram_name", new DrowizardHsitogramWrapper(dropwizardHistogram))
    }

    override def map(input: Long): Long = {
        meter.MarkEvent()
        ...do your work..
    }
}
```


## Meter
用来记录一个指标在某个时间段内的平均值。Flink 中的指标有 Task 算子中的 numRecordsInPerSecond,记录此 Task 或者算子每秒接收的记录数。

Flink没有提供默认的Meters收集器，需要借助Codahale/DropWizard meters实现。


```scala
class MyMapper extends RichMapFunction[Long, Long]{
    @transient private var meter: Meter  = _
    override def open(parameters: Configuration): Unit = {
        val dropwizardMeter = new com.codahale.metrics.Meter()
        meter =  getRuntimeContext()
            .getMetricGroup()
            .meter("your_meter_name", dropwizardMeter)
    }

    override def map(input: Long): Long = {
        meter.MarkEvent()
        ...do your work..
    }
}
```

# 监控指标报表
flink提供了非常丰富的监控指标Reporter，可以将采集到的架空指标推送到外部系统中。通过在conf/flink_conf.yaml中配置外部系统的Report即可。常见的Reporter有JMX、Graphite、Prometheus、StatsD、Datadog。此处不做介绍


# Backpressure 反压
反压是流式系统中一种非常重要的机制，主要作用是当系统中下游蒜子的处理速度下降，导致数据处理速率低于数据接入的速率时，通过反向背亚的方式让数据接入的速率下降，从而避免大量数据积压在flink中。

网络流控是为了在上下游速度不匹配的情况下，防止下游出现过载。
- 网络流控有静态限速和动态反压两种手段。
- Flink 1.5 之前是基于 TCP 流控 + bounded buffer 实现反压。
- Flink 1.5 之后实现了自己托管的 credit - based 流控机制，在应用层模拟 TCP 的流控机制。

[flink 网络流控原理](https://cloud.tencent.com/developer/article/1878476#:~:text=%E5%A6%82%E5%9B%BE%E6%89%80%E7%A4%BA%E5%9C%A8,%E4%B8%80%E4%B8%AACredit%20%E5%91%8A%E7%9F%A5%E4%BB%96%E5%8F%AF%E4%BB%A5)

## Backpressure 原因（常见）
- 数据倾斜： 数据倾斜问题是我们生产环境中出现频率最多的影响任务运行的因素，可以在 Flink 的后台管理页面看到每个 Task 处理数据的大小。当数据倾斜出现时，通常是简单地使用类似 KeyBy 等分组聚合函数导致的，需要用户将热点 Key 进行预处理，降低或者消除热点 Key 的影响。
- GC：垃圾回收问题也是造成反压的因素之一。不合理的设置 TaskManager 的垃圾回收参数会导致严重的 GC 问题，我们可以通过 -XX:+PrintGCDetails 参数查看 GC 的日志。
- 代码本身：开发者错误地使用 Flink 算子，没有深入了解算子的实现机制导致性能问题。我们可以通过查看运行机器节点的 CPU 和内存情况定位问题。
- 配置：比如恰当的内存设置也会导致反压

## Backpressure进程抽样
我们可以打开flink ui的Backpressure页签，触发反压数据的采集，反压过程对系统有一定的影响，主要因为JVM进程采样成本较高。Flink通过在TM中采样LocalBufferPool内存块上每个task的stack trace实现。默认情况下，TM会触发100次采样(每隔 50 ms 触发一次)，然后将采样的结果回报给JM，最终通过JM急性汇总计算，得出反压比例并在UI中展示。反压比例等于反压出现次数/采样次数。
- OK: 0 <= Ratio <= 0.10，正常；
- LOW: 0.10 < Ratio <= 0.5，一般；
- HIGH: 0.5 < Ratio <= 1，严重。

为了避免频繁采样，即便页面刷新，也要等待60s后才能触发一次Sampling过程。


当出现频繁反压，可以适当增加subtask并发度或者降低数据生产速度。

## Backpressure 配置
- web.backpressure.cleanup-interval: 反压数据集频率限制，默认60s
- web.backpressure.delay-between-samples：每次抽样到确认反压状态之间的时延，默认50ms
- web.backpressure.num-samples：每次的抽样数，默认100

# Checkpoint 监控优化
flink ui上的checkpoint页面有overview、hsitory、summary、configuration四个标签
- overview记录cp数据和最新记录，包括失败、完成的记录、cp次数、耗时等
- history记录历史触发的cp的情况
- summary记录所有完成的cp的统计的指标，比如端到端的持续时间、状态大小、分配过程中缓冲的数据大小
- configuration记录cp的基本配置
  - mode：Exactly Once 或者At Least Once
  - Interval：间隔
  - Timeout：超时
  - Minimum Pause：两个cp之间的最短时间
  - Persisit cp Externally：外部持久化配置

## Checkpoint 优化
flink会周期性进行cp。如果cp耗时比较长（大于interval），那么会出现cp堆积、排队，造成资源浪费。可以设置cp间的间隔，即flink ui中的Interval：
```scala
// 10s
StreamExecutionEnvironment.getCheckpointConfig().setMinPauseBetweenCheckpoints(10*1000)
```


对于状态数据非常大的应用，cp的耗时一般较长，可以考虑使用增量cp、异步cp、数据压缩的方法。
```scala
executionConfig.SetUseSnapshotCompression(true)
```

Flink 分布式快照里面的一个核心的元素就是流屏障（stream barrier）。这些屏障会被插入(injected)到数据流中，并作为数据流的一部分随着数据流动。屏障并不会持有任何数据，而是和数据一样线性的流动。可以看到屏障将数据流分成了两部分数据（实际上是多个连续的部分），一部分是当前快照的数据，一部分下一个快照的数据。每个屏障会带有它的快照ID。这个快照的数据都在这个屏障的前面。从图上看，数据是从左向右移动（右边的先进入系统），那么快照n包含的数据就是右侧到下一个屏障（n-1）截止的数据，图中两个灰色竖线之间的部分，也就是part of checkpoint n。另外屏障并不会打断数的流动,因而屏障是非常轻量的。在同一个时刻，多个快照可以在同一个数据流中，这也就是说多个快照可以同时产生。


如果上游算子发出一个屏障，尾部算子需要等很久才能收到该屏障并完成Barrier对齐，此时系统就处于反压状态中。

默认的Checkpoint配置是支持Exactly-Once投递的，这样能保证在重启恢复时，所有算子的状态对任一条数据只处理一次。用上文的Checkpoint原理来说，使用Exactly-Once就是进行了Checkpoint Barrier对齐，因此会有一定的延迟。如果作业延迟小，那么应该使用At-Least-Once投递，不进行对齐，但某些数据会被处理多次。
```scala
// 使用At-Least-Once
env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.AT_LEAST_ONCE)
```

默认情况下一个作业只允许1个Checkpoint执行，如果某个Checkpoint正在进行，另外一个Checkpoint被启动，新的Checkpoint需要挂起等待。
```scala
// 最多同时进行3个Checkpoint
env.getCheckpointConfig.setMaxConcurrentCheckpoints(3)
```

Checkpoint的初衷是用来进行故障恢复，如果作业是因为异常而失败，Flink会保存远程存储上的数据；如果开发者自己取消了作业，远程存储上的数据都会被删除。如果开发者希望通过Checkpoint数据进行调试，自己取消了作业，同时希望将远程数据保存下来，需要设置为：
```scala
// 作业取消后仍然保存Checkpoint
env.getCheckpointConfig.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION)
```

默认情况下，如果Checkpoint过程失败，会导致整个应用重启，我们可以关闭这个功能，这样Checkpoint失败不影响作业的运行。
```scala
env.getCheckpointConfig.setFailOnCheckpointingErrors(false)
```


# 内存优化
[flink 内存管理](https://cloud.tencent.com/developer/article/1904694?from=article.detail.1878476)

大部分开源框架（hadoop、spark、storm）都是基于JVM运行，但是JVM的内存管理机制往往存在OOM的问题，影响系统稳定性。为了避免这种问题，部分开源开源框架实现了自己的内存管理，例如spark的Tungstem项目，从而减轻框架对JVM GC的依赖。

Flink也基于JVM实现了自己的内存管理，将JVM根据内存区分为Unmanned Heap、Flink Managed Heap、Network Buffers三个区域。在Fink内部对flink managed heap急性管理，在启动集群的过程中直接将堆内存初始化称memory pages pool，也就是将内存全部以二进制数据的方式存储在内存页面迟，形成虚拟内存空间。新创建的对象都是以序列化成二进制的方式存储在内存页面迟中，当完成计算后数据对象flink就会将page置空，而不是通过JVM进行gc，保证数据对象的创建用于不会超过JVM堆内存的大小。



## Flink内存配置
Flink有三种部署方式(这里不谈Flink on k8s)，一种为本地模式，一种为standalone模式，还有一种为yarn或者mesos模式，这三种模式中，用户必须要选择一种进行配置（本地模式除外），否则flink将无法启动，这意味着，用户需要从以下的无默认值的配置参数中选择一个给出明确的配置。

| tm                                                                      | jm                             | description                                |
| ----------------------------------------------------------------------- | ------------------------------ | ------------------------------------------ |
| taskmanager.memory.flink.size                                           | jobmanager.memory.flink.size   | flink总内存：jvm堆内存、托管内存、堆外内存 |
| taskmanager.memory.process.size                                         | jobmanager.memory.process.size | 进程总内存：运行jvm的内存+flink总内存      |
| taskmanager.memory.task.heap.size、taskmanager.memory.task.managed.size | jobmanager.memory.heap.size    | 堆内存                                     |


不建议同时设置进程总内存和 Flink 总内存。这可能会造成内存配置冲突，从而导致部署失败。额外配置其他内存部分时，同样需要注意可能产生的配置冲突

### Job manager
JM在flink中主要承担集群资源管理、接收任务、调度task、收集任务状态以及管理TM的功能，jm本身不直接参与数据的计算过程中。因此默认情况下堆内存（1G）已经够用，你也可以自定义JM的堆内存大小：
- jobmanager.heap.size: 1024M
- jobmanager.memory.flink.size: flink总内存，堆内+堆外
- jobmanager.memory.process.size: 进程总内存，包含了由 Flink 应用使用的内存（Flink 总内存）以及由运行 Flink 的 JVM 使用的内存。





### Task manager
tm作为flink的工作节点，所有任务的计算逻辑都执行在tm，因此对tm内存配置显得尤为重要：
- taskmanager.heap.size:默认1024m，如果在yarn集群中，tm取决于task manager container的内存大小，且yarn环境下一般会减掉一部分内存用于container的容错，因此tm能用的堆内存：min(taskmanager.heap.size, yarn manager container memory - container容错内存)
- taskmanager.jvm-exit-on-oom：是否因为JVM oom时停止，默认false
- taskmanager.memory.size: 设定tm的内存大小，默认为0，如果不设置，则会使用taskmanager.memory.fraction作为内存分配一句
- taskmanager.memory.fraction： 设定tm堆中取出network  buffers内存后的内存分配比例。该内存主要用于tm任务排序、缓冲中间结果，例如如果设定0.8，则tm保留80%的内存用于中间结果数据的缓存，20%的内存用于创建用户定义函数中的数据对象存储。仅在不设置taskmanager.memory.size时生效
- taskmanager.memory.offheap：设置是否开启堆外内存供TM或者network buffers使用
- taskmanager.memory.preallocate：设置是否在启动TM中直接分配tm Managed Heap
- taskmanager.numberOfTaskSlots: 每个tm分配的slot数量


### network buffer
network buffer是flink数据交互层中的关键内存资源，主要用于缓存分布式数据处理过程中的输入数据。例如在Repartioning和Broadcasting操作过程中，需要消耗大量的nb 对数据进行缓存，然后才能触发之后的操作。如果系统出现“Insufficient number of network buffers”，一般是因为nb配置过低导致的。可以通过配置内存比例的方式来设置nb的大小：
- taskmanager.network.memory.fraction: jvm中用于nb的内存比例
- taskmanager.network.memory.min：最小的nb内存大小，默认64M
- taskmanager.network.memory。max：最大的nb内存大小，默认1G
- taskmanager.memory.segment-size: 内存管理器和network栈使用的buffer大小，默认32k